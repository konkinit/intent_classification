{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 14:52:31.478453: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 14:52:31.595165: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-04 14:52:31.597784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/\n",
      "2023-03-04 14:52:31.597795: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 14:52:32.094706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/\n",
      "2023-03-04 14:52:32.094767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/\n",
      "2023-03-04 14:52:32.094772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "from typing import List\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "from src.data.request_data import getDataTF, getDataHF\n",
    "from src.data.processing_data import StackedFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./inputs_data/tokens.json\") as file:\n",
    "    input = json.load(file)\n",
    "\n",
    "model_id = input[\"model_id\"]\n",
    "hf_token = input[\"hf_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_dyda_da = input[\"url_dyda_da\"]\n",
    "# df_dyda_da = getDataHF(url_dyda_da)\n",
    "# getDataTF(\"dyda_da\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contexts, Labels = StackedFormat(\"dyda_da\", 5).get_contexts_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ok , that shall be arranged . sun-set hotel . may i help you ? hold on , please . let me check it for you . yes , you're right . you will keep it for 3 days . well , now i want to change the date from 24th to 28th . yes , i have booked a room for 24th . it's a double room .\",\n",
       " \"i know . i know . i'm really sorry . i lost my bag . i'll call the lost and found office . it's ten after six.we ' re late.but dinner is at six thirty . i didn't think of it.thank you.and i do apologize for being late . i'm sorry i'm so late . i had a really bad day .\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Contexts[2][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2., 3., 2., 3., 2.],\n",
       "         [2., 2., 2., 3., 2.],\n",
       "         [2., 1., 2., 0., 3.],\n",
       "         ...,\n",
       "         [1., 3., 2., 2., 0.],\n",
       "         [2., 2., 3., 2., 3.],\n",
       "         [0., 1., 3., 3., 2.]]),\n",
       " tensor([[3., 2., 3., 1., 1.],\n",
       "         [2., 3., 3., 2., 2.],\n",
       "         [3., 2., 2., 3., 2.],\n",
       "         [2., 2., 3., 2., 3.],\n",
       "         [2., 3., 2., 1., 3.],\n",
       "         [3., 1., 1., 1., 2.],\n",
       "         [1., 3., 1., 0., 1.],\n",
       "         [1., 0., 1., 1., 0.],\n",
       "         [0., 0., 1., 1., 2.],\n",
       "         [2., 1., 3., 3., 3.],\n",
       "         [2., 0., 1., 3., 3.],\n",
       "         [2., 2., 2., 3., 3.],\n",
       "         [1., 3., 2., 3., 0.],\n",
       "         [2., 3., 1., 0., 1.],\n",
       "         [0., 3., 2., 1., 1.],\n",
       "         [2., 3., 3., 2., 3.],\n",
       "         [2., 2., 3., 2., 3.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [3., 3., 2., 1., 2.],\n",
       "         [3., 3., 2., 3., 1.],\n",
       "         [3., 1., 2., 3., 2.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [1., 3., 3., 1., 2.],\n",
       "         [3., 2., 2., 2., 3.],\n",
       "         [3., 1., 2., 1., 1.],\n",
       "         [3., 1., 0., 1., 1.],\n",
       "         [2., 2., 3., 2., 1.],\n",
       "         [2., 3., 1., 1., 3.],\n",
       "         [1., 2., 3., 1., 1.],\n",
       "         [1., 3., 0., 0., 1.],\n",
       "         [2., 1., 2., 2., 2.],\n",
       "         [2., 3., 2., 2., 3.],\n",
       "         [2., 1., 0., 2., 2.],\n",
       "         [2., 2., 2., 3., 3.],\n",
       "         [1., 0., 3., 1., 3.],\n",
       "         [1., 2., 0., 1., 0.],\n",
       "         [3., 2., 1., 1., 0.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 3., 3.],\n",
       "         [1., 2., 3., 3., 1.],\n",
       "         [2., 1., 3., 0., 2.],\n",
       "         [3., 2., 1., 3., 2.],\n",
       "         [0., 1., 2., 2., 3.],\n",
       "         [3., 1., 1., 0., 2.],\n",
       "         [3., 1., 0., 1., 0.],\n",
       "         [3., 1., 0., 2., 2.],\n",
       "         [3., 2., 2., 2., 3.],\n",
       "         [3., 3., 1., 2., 2.],\n",
       "         [1., 0., 3., 2., 1.],\n",
       "         [2., 3., 1., 2., 3.],\n",
       "         [0., 3., 2., 1., 3.],\n",
       "         [0., 1., 0., 1., 3.],\n",
       "         [2., 1., 3., 2., 3.],\n",
       "         [2., 3., 3., 2., 2.],\n",
       "         [2., 3., 2., 2., 2.],\n",
       "         [2., 3., 3., 0., 1.],\n",
       "         [2., 1., 1., 0., 2.],\n",
       "         [1., 0., 3., 2., 2.],\n",
       "         [2., 1., 1., 2., 2.],\n",
       "         [1., 1., 0., 2., 0.],\n",
       "         [2., 2., 3., 2., 3.],\n",
       "         [2., 3., 2., 1., 3.],\n",
       "         [2., 3., 3., 0., 1.],\n",
       "         [2., 0., 3., 1., 1.],\n",
       "         [3., 2., 1., 3., 2.],\n",
       "         [3., 2., 3., 3., 2.],\n",
       "         [1., 2., 2., 3., 2.],\n",
       "         [0., 3., 1., 3., 3.],\n",
       "         [0., 3., 3., 2., 1.],\n",
       "         [1., 2., 2., 1., 0.],\n",
       "         [0., 2., 2., 3., 1.],\n",
       "         [0., 3., 2., 3., 1.],\n",
       "         [2., 2., 3., 3., 3.],\n",
       "         [2., 2., 2., 3., 1.],\n",
       "         [0., 1., 1., 1., 0.],\n",
       "         [2., 3., 0., 1., 2.],\n",
       "         [3., 3., 2., 2., 3.],\n",
       "         [1., 3., 2., 0., 1.],\n",
       "         [2., 2., 2., 3., 3.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [3., 2., 0., 3., 1.]]),\n",
       " tensor([[1., 3., 1., 0., 2.],\n",
       "         [2., 1., 2., 0., 2.],\n",
       "         [3., 3., 2., 2., 2.],\n",
       "         [1., 0., 2., 2., 2.],\n",
       "         [1., 1., 0., 1., 0.],\n",
       "         [1., 1., 2., 2., 3.],\n",
       "         [2., 1., 0., 0., 1.],\n",
       "         [2., 1., 3., 0., 3.],\n",
       "         [3., 1., 2., 3., 0.],\n",
       "         [3., 3., 1., 3., 2.],\n",
       "         [2., 3., 1., 0., 3.],\n",
       "         [1., 3., 0., 1., 0.],\n",
       "         [3., 3., 1., 0., 3.],\n",
       "         [3., 1., 0., 2., 3.],\n",
       "         [1., 0., 1., 1., 3.],\n",
       "         [2., 3., 2., 3., 3.],\n",
       "         [1., 3., 1., 3., 2.],\n",
       "         [2., 1., 3., 3., 1.],\n",
       "         [1., 2., 1., 0., 0.],\n",
       "         [2., 3., 2., 0., 1.],\n",
       "         [1., 2., 1., 2., 0.],\n",
       "         [2., 1., 3., 3., 0.],\n",
       "         [2., 3., 0., 2., 1.],\n",
       "         [2., 1., 3., 1., 0.],\n",
       "         [2., 2., 2., 3., 3.],\n",
       "         [1., 2., 3., 3., 1.],\n",
       "         [1., 3., 2., 3., 2.],\n",
       "         [1., 0., 1., 0., 3.],\n",
       "         [2., 3., 1., 0., 1.],\n",
       "         [3., 2., 3., 2., 2.],\n",
       "         [1., 2., 3., 3., 3.],\n",
       "         [2., 2., 3., 3., 3.],\n",
       "         [3., 2., 2., 2., 2.],\n",
       "         [2., 3., 2., 3., 2.],\n",
       "         [0., 1., 1., 2., 3.],\n",
       "         [3., 3., 2., 2., 2.],\n",
       "         [1., 3., 2., 2., 2.],\n",
       "         [0., 1., 2., 0., 1.],\n",
       "         [0., 2., 1., 1., 0.],\n",
       "         [2., 1., 1., 3., 0.],\n",
       "         [2., 2., 2., 2., 3.],\n",
       "         [2., 2., 3., 2., 2.],\n",
       "         [1., 1., 3., 3., 1.],\n",
       "         [1., 0., 2., 1., 3.],\n",
       "         [2., 2., 3., 2., 2.],\n",
       "         [2., 2., 3., 2., 2.],\n",
       "         [2., 2., 2., 2., 3.],\n",
       "         [2., 2., 2., 3., 2.],\n",
       "         [0., 2., 3., 1., 1.],\n",
       "         [3., 2., 0., 1., 3.],\n",
       "         [0., 1., 1., 0., 1.],\n",
       "         [1., 3., 3., 2., 1.],\n",
       "         [0., 3., 1., 2., 2.],\n",
       "         [1., 1., 0., 3., 2.],\n",
       "         [2., 2., 3., 2., 3.],\n",
       "         [2., 0., 1., 0., 1.],\n",
       "         [3., 0., 3., 1., 2.],\n",
       "         [2., 2., 2., 1., 0.],\n",
       "         [3., 2., 2., 3., 2.],\n",
       "         [2., 3., 2., 3., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 3., 2., 3., 3.],\n",
       "         [2., 3., 2., 2., 3.],\n",
       "         [3., 2., 2., 2., 3.],\n",
       "         [3., 2., 0., 2., 1.],\n",
       "         [1., 2., 0., 2., 2.],\n",
       "         [1., 0., 2., 2., 3.],\n",
       "         [2., 2., 3., 2., 3.],\n",
       "         [2., 3., 3., 2., 2.],\n",
       "         [3., 3., 2., 3., 2.],\n",
       "         [2., 1., 2., 0., 2.],\n",
       "         [2., 1., 0., 1., 0.],\n",
       "         [1., 0., 3., 1., 1.],\n",
       "         [2., 3., 2., 3., 3.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 3., 2., 2., 2.],\n",
       "         [2., 2., 2., 3., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [0., 2., 1., 1., 3.],\n",
       "         [0., 2., 2., 1., 1.],\n",
       "         [3., 1., 0., 1., 3.],\n",
       "         [3., 2., 2., 3., 2.],\n",
       "         [2., 2., 3., 2., 2.],\n",
       "         [0., 2., 1., 2., 2.],\n",
       "         [2., 0., 1., 3., 3.],\n",
       "         [2., 2., 3., 3., 2.],\n",
       "         [1., 0., 0., 2., 1.],\n",
       "         [2., 2., 3., 2., 2.],\n",
       "         [2., 0., 0., 1., 1.]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = BertTokenizerFast.from_pretrained(\\'bert-base-uncased\\')\\n\\ninput_ids_tensor = torch.tensor(tokenizer.batch_encode_plus(Contexts,\\n                                     max_length=50,\\n                                     padding=True,\\n                                     truncation=\"only_first\")[\"input_ids\"][:3])\\n\\nwith torch.no_grad():\\n    embedds = model(input_ids_tensor)[0]\\nembedds\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids_tensor = torch.tensor(tokenizer.batch_encode_plus(Contexts,\n",
    "                                     max_length=50,\n",
    "                                     padding=True,\n",
    "                                     truncation=\"only_first\")[\"input_ids\"][:3])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedds = model(input_ids_tensor)[0]\n",
    "embedds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[[-5.5844e-03,  2.9351e-02,  4.0798e-02,  ..., -3.0757e-03,\n",
      "           2.3811e-02,  2.1139e-02]],\n",
      "\n",
      "        [[ 2.0593e-02, -7.3245e-03,  5.1020e-02,  ..., -4.6680e-03,\n",
      "           3.1919e-02,  1.2761e-02]],\n",
      "\n",
      "        [[ 1.4871e-02,  3.0301e-03,  3.0222e-02,  ..., -3.6234e-02,\n",
      "          -6.4907e-03,  5.1779e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.5901e-02,  1.6732e-02,  6.4316e-02,  ...,  5.8688e-03,\n",
      "          -1.8238e-02,  2.6413e-02]],\n",
      "\n",
      "        [[ 3.0525e-02, -6.6683e-03,  5.2210e-02,  ...,  3.3508e-02,\n",
      "           1.1205e-02,  2.5979e-02]],\n",
      "\n",
      "        [[ 2.3695e-02, -4.2341e-06,  3.2229e-02,  ..., -1.1047e-03,\n",
      "           1.9255e-02,  1.8958e-02]]])\n"
     ]
    }
   ],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = Contexts[1][:10]\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "def _reshape_(x: torch.Tensor): return x.reshape(x.shape[0], 1, x.shape[1])\n",
    "sentence_embeddings = _reshape_(sentence_embeddings)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# embedding_matrix = model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def embedding(texts: List[str],\n",
    "#              model_id: str,\n",
    "#              hf_token: str):\n",
    "#    api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "#    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "#    response = requests.post(api_url, \n",
    "#                             headers=headers, \n",
    "#                             json={\"inputs\": texts, \n",
    "#                                   \"options\":{\"wait_for_model\":True}})\n",
    "#    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56d72c1f3e2875ebbfe306c77853982cb6a556161e151868ef87de4f1b5584b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
