{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 14:18:43.532170: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 14:18:44.239478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-12 14:18:44.239544: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-12 14:18:44.239551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "os.chdir(\"../\")\n",
    "from tqdm import tqdm\n",
    "from src.utils.get_datasets import getDataTF, getDataHF\n",
    "from src.data.processing_data import Format\n",
    "from src.models.pipelines import Pipeline\n",
    "from src.models.encoders import TransformersEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dataset = \"mrda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i guess .</td>\n",
       "      <td>Bed002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>okay we're on .</td>\n",
       "      <td>Bed002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so just make sure that th- - your wireless mik...</td>\n",
       "      <td>Bed002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check one .</td>\n",
       "      <td>Bed002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check one .</td>\n",
       "      <td>Bed002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance Dialogue_ID  Label\n",
       "0                                          i guess .      Bed002      0\n",
       "1                                    okay we're on .      Bed002      0\n",
       "2  so just make sure that th- - your wireless mik...      Bed002      0\n",
       "3                                        check one .      Bed002      0\n",
       "4                                        check one .      Bed002      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"./inputs_data/data_{_dataset}_train.csv\", encoding=\"utf-8\",  sep=\"|\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of each dialog acts \n",
    "# df.value_counts(\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct label i the dataset\n",
    "len(df[\"Label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct dialogue i the dataset\n",
    "len(df[\"Dialogue_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of utterance per dialogue\n",
    "int(df[[\"Dialogue_ID\", \"Label\"]].groupby(\"Dialogue_ID\").count().Label.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 14:18:47.810609: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 14:18:48.534663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13581 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:3b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dimDialogAct, Contexts, Labels = Format(_dataset, 50, \"spread\").get_contexts_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([56, 50, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Contexts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformersEncoder('xlnet-base-cased', \"spread\", 50).batch_embedding(Contexts[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Pipeline working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 998/998 [07:30<00:00,  2.21it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 115/115 [00:50<00:00,  2.27it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 11/11 [00:04<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "63/63 [==============================] - 2s 16ms/step - loss: 0.0999 - val_loss: 0.0849\n",
      "Epoch 2/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0785 - val_loss: 0.0777\n",
      "Epoch 3/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0749 - val_loss: 0.0731\n",
      "Epoch 4/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0698 - val_loss: 0.0708\n",
      "Epoch 5/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0699 - val_loss: 0.0705\n",
      "Epoch 6/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0630 - val_loss: 0.0620\n",
      "Epoch 7/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0592 - val_loss: 0.0601\n",
      "Epoch 8/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0577 - val_loss: 0.0630\n",
      "Epoch 9/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0591 - val_loss: 0.0583\n",
      "Epoch 10/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0559 - val_loss: 0.0580\n",
      "Epoch 11/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0571 - val_loss: 0.0610\n",
      "Epoch 12/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0558 - val_loss: 0.0570\n",
      "Epoch 13/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0540 - val_loss: 0.0560\n",
      "Epoch 14/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0534 - val_loss: 0.0562\n",
      "Epoch 15/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0552 - val_loss: 0.0556\n",
      "Epoch 16/500\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.0526 - val_loss: 0.0551\n",
      "Epoch 17/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0520 - val_loss: 0.0545\n",
      "Epoch 18/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0524 - val_loss: 0.0540\n",
      "Epoch 19/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0516 - val_loss: 0.0539\n",
      "Epoch 20/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0542\n",
      "Epoch 21/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0513 - val_loss: 0.0529\n",
      "Epoch 22/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0508 - val_loss: 0.0528\n",
      "Epoch 23/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0505 - val_loss: 0.0541\n",
      "Epoch 24/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0542 - val_loss: 0.0529\n",
      "Epoch 25/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0514 - val_loss: 0.0583\n",
      "Epoch 26/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0517 - val_loss: 0.0536\n",
      "Epoch 27/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0497 - val_loss: 0.0522\n",
      "Epoch 28/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0505 - val_loss: 0.0520\n",
      "Epoch 29/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0495 - val_loss: 0.0519\n",
      "Epoch 30/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0491 - val_loss: 0.0520\n",
      "Epoch 31/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0485 - val_loss: 0.0513\n",
      "Epoch 32/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0481 - val_loss: 0.0511\n",
      "Epoch 33/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0480 - val_loss: 0.0506\n",
      "Epoch 34/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0479 - val_loss: 0.0508\n",
      "Epoch 35/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0476 - val_loss: 0.0506\n",
      "Epoch 36/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0511 - val_loss: 0.0526\n",
      "Epoch 37/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0493 - val_loss: 0.0511\n",
      "Epoch 38/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0474 - val_loss: 0.0503\n",
      "Epoch 39/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0472 - val_loss: 0.0505\n",
      "Epoch 40/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0470 - val_loss: 0.0502\n",
      "Epoch 41/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0473 - val_loss: 0.0499\n",
      "Epoch 42/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0466 - val_loss: 0.0495\n",
      "Epoch 43/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0465 - val_loss: 0.0497\n",
      "Epoch 44/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0506 - val_loss: 0.0497\n",
      "Epoch 45/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0467 - val_loss: 0.0492\n",
      "Epoch 46/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0464 - val_loss: 0.0498\n",
      "Epoch 47/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0462 - val_loss: 0.0491\n",
      "Epoch 48/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0459 - val_loss: 0.0491\n",
      "Epoch 49/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0458 - val_loss: 0.0495\n",
      "Epoch 50/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0459 - val_loss: 0.0491\n",
      "Epoch 51/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0454 - val_loss: 0.0487\n",
      "Epoch 52/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0462 - val_loss: 0.0494\n",
      "Epoch 53/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0509 - val_loss: 0.0499\n",
      "Epoch 54/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0461 - val_loss: 0.0492\n",
      "Epoch 55/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0457 - val_loss: 0.0493\n",
      "Epoch 56/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0455 - val_loss: 0.0485\n",
      "Epoch 57/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0465 - val_loss: 0.0510\n",
      "Epoch 58/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0456 - val_loss: 0.0490\n",
      "Epoch 59/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0459 - val_loss: 0.0494\n",
      "Epoch 60/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0466 - val_loss: 0.0494\n",
      "Epoch 61/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0457 - val_loss: 0.0491\n",
      "Epoch 62/500\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0452 - val_loss: 0.0490\n",
      "Epoch 63/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0460 - val_loss: 0.0497\n",
      "Epoch 64/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0742 - val_loss: 0.0718\n",
      "Epoch 65/500\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 66/500\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0578 - val_loss: 0.0570\n",
      "1/1 [==============================] - 0s 260ms/step\n"
     ]
    }
   ],
   "source": [
    "df_report = Pipeline(\n",
    "    \"swda\",\n",
    "    \"spread\",\n",
    "    50,\n",
    "    'bert-base-uncased',\n",
    "    \"GRU\").summary_exec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>encoder_model</th>\n",
       "      <th>decoder_model</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>swda</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.694545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name      encoder_model decoder_model  performance\n",
       "0         swda  bert-base-uncased           GRU     0.694545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fa046f995eb80ac40c0869a1f9df46519f4ada8b8c395ef25dd1aa1a1a2fc63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
