{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 20:51:23.832494: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 20:51:23.932991: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-02 20:51:23.935793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/\n",
      "2023-03-02 20:51:23.935805: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-02 20:51:24.454087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/\n",
      "2023-03-02 20:51:24.454151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/:/home/ikonkobo/anaconda3/lib/\n",
      "2023-03-02 20:51:24.454156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "from typing import List\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "from src.data.request_data import getDataTF, getDataHF\n",
    "from src.data.processing_data import StackedFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./inputs_data/tokens.json\") as file:\n",
    "    input = json.load(file)\n",
    "\n",
    "model_id = input[\"model_id\"]\n",
    "hf_token = input[\"hf_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_dyda_da = input[\"url_dyda_da\"]\n",
    "# df_dyda_da = getDataHF(url_dyda_da)\n",
    "# getDataTF(\"dyda_da\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contexts, Labels = StackedFormat(\"dyda_da\", \"train\", 5).get_contexts_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"that's true , but then you have to buy a record player . can you study with the radio on ? no , i listen to background music . what is the difference ? the radio has too many comerials .\",\n",
       " \"wow ! you're really working up a storm ! i know . i've even worked up a sweat . you look like a cooking show host--only messier . anyone home ? jen ! i'm in the kitchen ... let yourself in !\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Contexts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 2., 3., 2.],\n",
       "        [2., 2., 2., 3., 2.],\n",
       "        [2., 1., 2., 0., 3.],\n",
       "        ...,\n",
       "        [1., 3., 2., 2., 0.],\n",
       "        [2., 2., 3., 2., 3.],\n",
       "        [0., 1., 3., 3., 2.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = BertTokenizerFast.from_pretrained(\\'bert-base-uncased\\')\\n\\ninput_ids_tensor = torch.tensor(tokenizer.batch_encode_plus(Contexts,\\n                                     max_length=50,\\n                                     padding=True,\\n                                     truncation=\"only_first\")[\"input_ids\"][:3])\\n\\nwith torch.no_grad():\\n    embedds = model(input_ids_tensor)[0]\\nembedds\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids_tensor = torch.tensor(tokenizer.batch_encode_plus(Contexts,\n",
    "                                     max_length=50,\n",
    "                                     padding=True,\n",
    "                                     truncation=\"only_first\")[\"input_ids\"][:3])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedds = model(input_ids_tensor)[0]\n",
    "embedds\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 0.0083,  0.0223,  0.0323,  ...,  0.0124,  0.0278,  0.0150],\n",
      "        [ 0.0267, -0.0055,  0.0405,  ...,  0.0071,  0.0359,  0.0112],\n",
      "        [ 0.0153,  0.0066,  0.0517,  ..., -0.0032,  0.0127,  0.0094],\n",
      "        ...,\n",
      "        [ 0.0113, -0.0126,  0.0788,  ...,  0.0023,  0.0367, -0.0040],\n",
      "        [ 0.0142,  0.0099,  0.0375,  ...,  0.0171,  0.0337,  0.0328],\n",
      "        [ 0.0172, -0.0161,  0.0239,  ..., -0.0110,  0.0151,  0.0002]])\n"
     ]
    }
   ],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = Contexts[:10]\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# embedding_matrix = model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def embedding(texts: List[str],\n",
    "#              model_id: str,\n",
    "#              hf_token: str):\n",
    "#    api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "#    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "#    response = requests.post(api_url, \n",
    "#                             headers=headers, \n",
    "#                             json={\"inputs\": texts, \n",
    "#                                   \"options\":{\"wait_for_model\":True}})\n",
    "#    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56d72c1f3e2875ebbfe306c77853982cb6a556161e151868ef87de4f1b5584b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
